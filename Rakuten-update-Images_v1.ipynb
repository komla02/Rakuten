{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réduction de Dimensionnalité et Analyse d'Image avec PCA, UMAP et RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook initie par une phase de préparation des données, avant de s'engager dans des méthodes de réduction de dimension via PCA et UMAP. Il procède ensuite à l'entraînement d'un modèle RandomForest sur l'ensemble de données original ainsi que sur celui transformé par PCA, évaluant leur performance à travers des rapports de classification et des matrices de confusion. En conclusion, il met en œuvre une visualisation des données transformées dans un espace bidimensionnel grâce à PCA et UMAP, démontrant ainsi l'utilité de ces méthodes pour la visualisation et l'optimisation potentielle de la performance des modèles de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\user\\anaconda3\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
      "     -------------------------------------- 85.7/85.7 kB 967.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from umap-learn) (0.55.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from umap-learn) (4.64.1)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn) (63.4.1)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from numba>=0.51.2->umap-learn) (0.38.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->umap-learn) (0.4.5)\n",
      "Installing collected packages: pynndescent, umap-learn\n",
      "Successfully installed pynndescent-0.5.12 umap-learn-0.5.6\n"
     ]
    }
   ],
   "source": [
    "# Installation des dépendances nécessaires\n",
    "!pip install opencv-python pandas matplotlib scikit-learn umap-learn\n",
    "\n",
    "# Importation des modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration des chemins et chargement des données\n",
    "path = \"C:/Users/user/OneDrive/DATASIENCETEST/PROJET/Data/Update/images/image_train\"\n",
    "\n",
    "# Chargement des métadonnées et des étiquettes\n",
    "X = pd.read_csv(r\"C:\\Users\\user\\OneDrive\\DATASIENCETEST\\PROJET\\Data\\Update\\X_train_update.csv\", index_col=0)\n",
    "y = pd.read_csv(r\"C:\\Users\\user\\OneDrive\\DATASIENCETEST\\PROJET\\Data\\Update\\Y_train_CVw08PX.csv\", index_col=0).squeeze().map(str)\n",
    "\n",
    "# Ajout du chemin complet vers chaque image dans le DataFrame X\n",
    "X['image_path'] = X.apply(lambda row: os.path.join(path, f'image_{row.imageid}_product_{row.productid}.jpg'), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des images\n",
    "\n",
    "# Paramètres pour le redimensionnement\n",
    "image_size = (50, 50)\n",
    "\n",
    "# Fonction pour lire et redimensionner une image\n",
    "def process_image(image_path):\n",
    "    img_gray = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_gray_resized = cv2.resize(img_gray, image_size, interpolation=cv2.INTER_AREA)\n",
    "    return img_gray_resized\n",
    "\n",
    "# Liste des chemins d'images à traiter\n",
    "path_list = X['image_path'].tolist()\n",
    "\n",
    "# Utilisation de ThreadPoolExecutor pour traiter les images en parallèle\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    images_resized = list(executor.map(process_image, path_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construction des données du DataFrame\n",
    "\n",
    "# Fonction pour convertir les images en une liste de valeurs de pixels\n",
    "def image_to_pixel_list(image, idx, label):\n",
    "    pixel_list = [idx, label]\n",
    "    for row in image:\n",
    "        for pixel in row:\n",
    "            pixel_list.append(pixel)\n",
    "    return pixel_list\n",
    "\n",
    "# Préparation des données pour le DataFrame\n",
    "data_for_df = [image_to_pixel_list(image, idx, label) for image, idx, label in zip(images_resized, X.index, y)]\n",
    "\n",
    "# Création des noms de colonnes\n",
    "column_names = ['index', 'label'] + [f'pixel{i+1}' for i in range(image_size[0] * image_size[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du DataFrame\n",
    "\n",
    "# Initialisation du DataFrame vide avec des types spécifiés pour économiser de la mémoire\n",
    "column_types = {f'pixel{i}': 'uint8' for i in range(1, image_size[0] * image_size[1] + 1)}\n",
    "column_types['index'] = int\n",
    "column_types['label'] = str\n",
    "\n",
    "# Préparation d'une liste pour recueillir les DataFrames de lots\n",
    "df_list = []\n",
    "\n",
    "# Taille du lot pour le traitement par morceaux\n",
    "batch_size = 5000\n",
    "\n",
    "# Traitement par lots\n",
    "for start in range(0, len(data_for_df), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch_data = data_for_df[start:end]\n",
    "    batch_df = pd.DataFrame(batch_data, columns=column_names)\n",
    "    # Assurez-vous que les types sont corrects\n",
    "    batch_df = batch_df.astype(column_types)\n",
    "    df_list.append(batch_df)\n",
    "\n",
    "# Concaténez tous les DataFrames de la liste en une seule opération\n",
    "rakuten_images_df = pd.concat(df_list, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rakuten_images_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du DataFrame en CSV\n",
    "rakuten_images_df.to_csv('rakuten_images_processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Charger le df stocké:\n",
    "\n",
    "rakuten_images_processed= pd.read_csv('rakuten_images_processed.csv')\n",
    "rakuten_images_processed.drop(columns=['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rakuten_images_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation des variables et target:\n",
    "target = rakuten_images_processed['label']\n",
    "data = rakuten_images_processed.drop('label',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse de l'impact de la réduction de variables sur la variance du dataset:\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import itertools # Pour créer des iterateurs\n",
    "from sklearn.metrics import f1_score, make_scorer, classification_report\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim(0,100)\n",
    "plt.plot(pca.explained_variance_ratio_);\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim(0,100)\n",
    "plt.axhline(y = 0.90, color ='r', linestyle = '--')\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Réduction de dimensionnalité avec PCA\n",
    "pca = PCA(n_components = 0.90)\n",
    "pca.fit(data)\n",
    "print(\"Nombre de composantes retenues :\", pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer la part de variance expliquée par les composantes PCA retenues\n",
    "print(\"La part de variance expliquée est\", round(pca.explained_variance_ratio_.sum(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Projection des données dans le nouveau space avec 99 varaibles\n",
    "\n",
    "X_train = data[:67933] #67933=84916*80%\n",
    "X_test = data[67933:]\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y[:67933]\n",
    "y_test = y[67933:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement d'un modèle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'une fonction propre pour la confusion matrix\n",
    "\n",
    "#Variable target avec les labels organisées \n",
    "y_organised = ['10','2280','2403','2705','2522',\n",
    "               '40','50','2905','2462','60',\n",
    "               '1280','1281','1300','1180','1140','1160',\n",
    "               '1320','1560',\n",
    "               '2582','2583','2585','1302','2220',\n",
    "               '1920','2060',\n",
    "               '1301','1940'\n",
    "              ]\n",
    "\n",
    "def conf_matx(y_test,y_pred):\n",
    "\n",
    "    cnf_matrix = metrics.confusion_matrix(y_test,y_pred,labels=y_organised)\n",
    "    y_organised\n",
    "\n",
    "    pond_matrix = []\n",
    "    for line in cnf_matrix:\n",
    "        pond_line = []\n",
    "        for cell in line:\n",
    "            pond_line.append(round(cell/sum(line),2))\n",
    "        pond_matrix.append(pond_line)\n",
    "        #print(sum(line))\n",
    "        #print(sum(pond_line))\n",
    "    cnf_matrix = np.array(pond_matrix)\n",
    "\n",
    "    ###Optionnel: Afficher une matrice de confusion sous forme de tableau coloré\n",
    "    #classes = set(y_pred)\n",
    "    classes = y_organised\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    plt.imshow(cnf_matrix, interpolation='nearest',cmap='Blues')\n",
    "    plt.title(\"Matrice de confusion\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    #tick_marks = set(y_test)\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j, i, cnf_matrix[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cnf_matrix[i, j] > ( cnf_matrix.max() / 2) else \"black\")\n",
    "\n",
    "    plt.ylabel('Vrais labels')\n",
    "    plt.xlabel('Labels prédits')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation du modèle sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_matx(y_test,y_pred) #montre les résultats en pourcentages!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset reduit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_red = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "clf_red.fit(X_train_pca, y_train)\n",
    "clf_red.score(X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_red.predict(X_test_pca)\n",
    "print(classification_report(y_test, y_pred))\n",
    "conf_matx(y_test,y_pred) #montre les résultats en pourcentages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction de dimensionnalité avec PCA à deux composantes pour visualisation\n",
    "pca = PCA(n_components = 2)\n",
    "data_2D = pca.fit_transform(data)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(data_2D[:, 0], data_2D[:, 1], c = target, cmap=plt.cm.Spectral)\n",
    "\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_ylabel('PCA 2')\n",
    "\n",
    "ax.set_title(\"Données projetées sur les 2 axes de PCA\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install umap-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction de dimensionnalité avec UMAP pour la visualisation\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=50, n_components=2, random_state=42)\n",
    "dataUMAP = reducer.fit_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des données réduites avec UMAP\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(dataUMAP[:, 0], dataUMAP[:, 1], c=target, cmap=plt.cm.Spectral, alpha=.7, s=4)\n",
    "\n",
    "ax.set_xlabel('UMAP1')\n",
    "ax.set_ylabel('UMAP2')\n",
    "\n",
    "ax.set_title(\"Manifold 2D identifié par la UMAP\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "\n",
    "#Fonction pour visualiser des données en 2D avec des images comme annotations.\n",
    "\n",
    "def plot_components(data, model, images=None, ax=None, thumb_frac=0.05, cmap='gray_r', prefit=False):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Transforme les données si prefit=False, sinon utilise les données projetées directement\n",
    "    if not prefit:\n",
    "        proj = model.fit_transform(data)\n",
    "    else:\n",
    "        proj = data\n",
    "    ax.plot(proj[:, 0], proj[:, 1], '.b')  # Dessine les points projetés\n",
    "    \n",
    "    # Si des images sont fournies, les ajoute au graphique\n",
    "    if images is not None:\n",
    "        min_dist_2 = (thumb_frac * max(proj.max(0) - proj.min(0))) ** 2\n",
    "        shown_images = np.array([2 * proj.max(0)])  # Initialise avec une valeur hors portée\n",
    "        for i in range(data.shape[0]):  # Itère sur chaque point projeté\n",
    "            dist = np.sum((proj[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < min_dist_2:  # Vérifie la distance minimum pour éviter le surpeuplement\n",
    "                continue  # Passe au point suivant si trop proche\n",
    "            shown_images = np.vstack([shown_images, proj[i]])  # Ajoute le point à la liste des points affichés\n",
    "            imagebox = AnnotationBbox(OffsetImage(images[i], cmap=cmap), proj[i], frameon=False)  # Crée une boîte d'annotation pour l'image\n",
    "            ax.add_artist(imagebox)  # Ajoute l'image au graphique\n",
    "\n",
    "\n",
    "#visualiser les résultats de la réduction de dimension avec UMAP\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "plot_components(data = dataUMAP, model = reducer, images=data.values.reshape((-1, 50, 50)),\n",
    "                ax=ax, thumb_frac=0.05, prefit = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
